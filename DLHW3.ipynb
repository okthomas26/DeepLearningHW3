{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d842f653-d503-40a3-93bb-97734a8ff20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a2ddb9-bd62-4e82-9581-283e2ed29826",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostonyt/.local/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc8c10cc-d5e0-4aaa-a21e-d2dd8ef85549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_encode_squad(file_path, tokenizer):\n",
    "    contexts, questions, answers = [], [], []\n",
    "    \n",
    "    # Load and parse the SQuAD data\n",
    "    with open(file_path, 'r') as file:\n",
    "        squad_data = json.load(file)\n",
    "    \n",
    "    for article in squad_data['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                answer_type = 'plausible_answers' if 'plausible_answers' in qa else 'answers'\n",
    "                \n",
    "                for answer in qa[answer_type]:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    # Track answer text and its character positions\n",
    "                    answers.append({\n",
    "                        'text': answer['text'],\n",
    "                        'start': answer['answer_start'],\n",
    "                        'end': answer['answer_start'] + len(answer['text'])\n",
    "                    })\n",
    "\n",
    "    # Tokenize contexts and questions together\n",
    "    encodings = tokenizer(contexts, questions, truncation=True, padding=True, stride=100)\n",
    "    \n",
    "    # Initialize lists for token start and end positions\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    # Calculate token positions for each answer\n",
    "    for i, answer in enumerate(answers):\n",
    "        start_idx = encodings.char_to_token(i, answer['start']) or tokenizer.model_max_length\n",
    "        end_idx = encodings.char_to_token(i, max(0, answer['end'] - 1)) or tokenizer.model_max_length\n",
    "        \n",
    "        # Adjust end position if out of range\n",
    "        shift = 1\n",
    "        while end_idx is None:\n",
    "            end_idx = encodings.char_to_token(i, max(0, answer['end'] - 1 - shift))\n",
    "            shift += 1\n",
    "        \n",
    "        start_positions.append(start_idx)\n",
    "        end_positions.append(end_idx)\n",
    "\n",
    "    # Add start and end positions to encodings and remove 'token_type_ids'\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    encodings.pop('token_type_ids', None)\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "# Load, encode, and add positions in one step\n",
    "train_encodings = load_and_encode_squad('spokenSquad/train-v1.json', tokenizer)\n",
    "test_encodings = load_and_encode_squad('spokenSquad/test-v1.json', tokenizer)\n",
    "\n",
    "# Dataset class remains the same\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "test_dataset = SquadDataset(test_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abce642b-6ef6-4d8e-a55f-d5a5c1bffd96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  6549,  2135,  1996,  2082,  2038,  1037,  3234,  2839,  1012,\n",
      "        10234,  1996,  2364,  2311,  2082,  8514,  2003,  1996,  3585,  6231,\n",
      "         1997,  1996,  6261,  2984,  1012,  3202,  1999,  2392,  1997,  1996,\n",
      "         2364,  2311,  1999,  5307,  2009,  2003,  1037,  6967,  6231,  1997,\n",
      "         4828,  2007,  2608, 10439, 14995,  6924,  2007,  1996,  5722,  1998,\n",
      "         1996,  2919,  2033,  5004,  3415,  1012,  2279,  2000,  1996,  2364,\n",
      "         2311,  2003,  1996, 13546,  1997,  1996,  6730,  2540,  1012,  3202,\n",
      "         2369,  1996, 13546,  2003,  1996, 24665, 23052, 10047,  2984,  1999,\n",
      "         2173,  1997,  7083,  1998,  9185,  1012,  2009,  2003,  1037, 15059,\n",
      "         1997,  1996, 24665, 23052,  2012, 10223, 26371,  2605,  2073,  1996,\n",
      "         6261,  2984, 22353,  2135,  2596,  2000,  2358, 16595,  9648,  4674,\n",
      "         2145,  5255,  7763,  5595,  2809,  1012,  2012,  1996,  2203,  1997,\n",
      "         1996,  2364,  3298,  1998,  1999,  1037,  3622,  2240,  2008,  8539,\n",
      "         2083,  2093, 11342,  1999,  1996,  2751,  8514,  2003,  2004,  3722,\n",
      "         2715,  2962,  6231,  1997,  2984,  1012,   102,  2054,  2003,  1999,\n",
      "         2392,  1997,  1996, 10289,  8214,  2364,  2311,  1029,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'start_positions': tensor(36), 'end_positions': tensor(40)}\n",
      "Decoded context and question: architecturally the school has a catholic character. atop the main building school dome is the golden statue of the virgin mary. immediately in front of the main building in facing it is a copper statue of christ with arms appraised with the legend and the bad meow names. next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto im mary in place of prayer and reflection. it is a replica of the grotto at lourdes france where the virgin mary reputedly appeared to st bernadette still burning eighteen fifty eight. at the end of the main drive and in a direct line that connects through three statues in the gold dome is as simple modern stone statue of mary. what is in front of the notre dame main building?\n",
      "Start position: 36\n",
      "End position: 40\n"
     ]
    }
   ],
   "source": [
    "#Sneak a peak\n",
    "example = train_dataset[0]\n",
    "print(example)\n",
    "\n",
    "# Decode \n",
    "input_ids = example['input_ids']\n",
    "text_decoded = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "print(\"Decoded context and question:\", text_decoded)\n",
    "print(\"Start position:\", example['start_positions'].item())\n",
    "print(\"End position:\", example['end_positions'].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ebd22b0-3768-4b22-983c-dc7fe0fee80a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|██████████| 1160/1160 [09:06<00:00,  2.12it/s, loss=0.401]\n",
      "Epoch 2/3: 100%|██████████| 1160/1160 [09:05<00:00,  2.13it/s, loss=0.356] \n",
      "Epoch 3/3: 100%|██████████| 1160/1160 [09:05<00:00,  2.13it/s, loss=0.0981]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# Set device and enable mixed precision\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and gradient scaler for mixed precision\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Set up data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "warmup_steps = int(0.1 * num_training_steps)\n",
    "\n",
    "# Scheduler for learning rate warm-up\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n",
    "\n",
    "# Gradient accumulation settings\n",
    "accumulation_steps = 2  # Accumulate gradients for 2 batches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    loop = tqdm(train_loader, leave=True, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    \n",
    "    for step, batch in enumerate(loop):\n",
    "        # Move batch to device with asynchronous loading\n",
    "        input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "        attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "        start_positions = batch['start_positions'].to(device, non_blocking=True)\n",
    "        end_positions = batch['end_positions'].to(device, non_blocking=True)\n",
    "        \n",
    "        # Mixed precision forward pass with gradient accumulation\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                            start_positions=start_positions, end_positions=end_positions)\n",
    "            loss = outputs.loss / accumulation_steps  # Scale loss for gradient accumulation\n",
    "            \n",
    "        # Backward pass\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Update weights and zero gradients every 'accumulation_steps' batches\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "\n",
    "        # Update progress bar with current loss\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d78819f-ddd9-4eae-bc56-a60bcf720707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 993/993 [06:08<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the validation data loader\n",
    "val_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Create arrays to hold true and predicted values\n",
    "true_starts = []\n",
    "true_ends = []\n",
    "pred_starts = []\n",
    "pred_ends = []\n",
    "\n",
    "# Progress bar for the validation loop\n",
    "loop = tqdm(val_loader, desc=\"Evaluating\")\n",
    "for batch in loop:\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Move batch data to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "\n",
    "        # Model predictions\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        # Store true and predicted values\n",
    "        true_starts.append(start_true.cpu().numpy())\n",
    "        true_ends.append(end_true.cpu().numpy())\n",
    "        pred_starts.append(start_pred.cpu().numpy())\n",
    "        pred_ends.append(end_pred.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_starts = np.concatenate(true_starts)\n",
    "true_ends = np.concatenate(true_ends)\n",
    "pred_starts = np.concatenate(pred_starts)\n",
    "pred_ends = np.concatenate(pred_ends)\n",
    "\n",
    "# Function to calculate F1 score\n",
    "def calculate_f1(true_labels, predicted_labels):\n",
    "    true_pos = np.sum(true_labels == predicted_labels)\n",
    "    false_pos = np.sum((predicted_labels != true_labels) & (predicted_labels != -1))\n",
    "    false_neg = np.sum((true_labels != predicted_labels) & (true_labels != -1))\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos + 1e-9)\n",
    "    recall = true_pos / (true_pos + false_neg + 1e-9)\n",
    "    \n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "\n",
    "# Calculate F1 scores for start and end predictions\n",
    "f1_start = calculate_f1(true_starts, pred_starts)\n",
    "f1_end = calculate_f1(true_ends, pred_ends)\n",
    "\n",
    "# Average F1 score\n",
    "f1_average = (f1_start + f1_end) / 2\n",
    "\n",
    "print(f\"F1 score: {f1_average:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94af9c89-4605-4135-81d8-721e171c8598",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('DLHW3/modified_tokenizer/tokenizer_config.json',\n",
       " 'DLHW3/modified_tokenizer/special_tokens_map.json',\n",
       " 'DLHW3/modified_tokenizer/vocab.txt',\n",
       " 'DLHW3/modified_tokenizer/added_tokens.json',\n",
       " 'DLHW3/modified_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the modified model\n",
    "model.save_pretrained( \"DLHW3/improved_model\" )\n",
    "\n",
    "# If you want to save the tokenizer as well\n",
    "tokenizer.save_pretrained(\"DLHW3/modified_tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
