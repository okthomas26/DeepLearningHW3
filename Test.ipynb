{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c87845-e096-4d14-9fd1-715c2cecb312",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import os\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('DLHW3/improved_model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('DLHW3/modified_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ab2d5c5-fa9c-42d1-92ac-1325f3446093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_and_encode_squad(file_path, tokenizer):\n",
    "    contexts, questions, answers = [], [], []\n",
    "    \n",
    "    # Load and parse the SQuAD data\n",
    "    with open(file_path, 'r') as file:\n",
    "        squad_data = json.load(file)\n",
    "    \n",
    "    for article in squad_data['data']:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                answer_type = 'plausible_answers' if 'plausible_answers' in qa else 'answers'\n",
    "                \n",
    "                for answer in qa[answer_type]:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    # Track answer text and its character positions\n",
    "                    answers.append({\n",
    "                        'text': answer['text'],\n",
    "                        'start': answer['answer_start'],\n",
    "                        'end': answer['answer_start'] + len(answer['text'])\n",
    "                    })\n",
    "\n",
    "    # Tokenize contexts and questions together\n",
    "    encodings = tokenizer(contexts, questions, truncation=True, padding=True, stride=100)\n",
    "    \n",
    "    # Initialize lists for token start and end positions\n",
    "    start_positions, end_positions = [], []\n",
    "\n",
    "    # Calculate token positions for each answer\n",
    "    for i, answer in enumerate(answers):\n",
    "        start_idx = encodings.char_to_token(i, answer['start']) or tokenizer.model_max_length\n",
    "        end_idx = encodings.char_to_token(i, max(0, answer['end'] - 1)) or tokenizer.model_max_length\n",
    "        \n",
    "        # Adjust end position if out of range\n",
    "        shift = 1\n",
    "        while end_idx is None:\n",
    "            end_idx = encodings.char_to_token(i, max(0, answer['end'] - 1 - shift))\n",
    "            shift += 1\n",
    "        \n",
    "        start_positions.append(start_idx)\n",
    "        end_positions.append(end_idx)\n",
    "\n",
    "    # Add start and end positions to encodings and remove 'token_type_ids'\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    encodings.pop('token_type_ids', None)\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "# Load, encode, and add positions in one step\n",
    "train_encodings = load_and_encode_squad('spokenSquad/train-v1.json', tokenizer)\n",
    "test_encodings = load_and_encode_squad('spokenSquad/test-v1.json', tokenizer)\n",
    "\n",
    "# Dataset class remains the same\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "test_dataset = SquadDataset(test_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a494c875-c24c-4696-be05-6d27d3ef8a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 993/993 [06:08<00:00,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "# Switch model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize the validation data loader\n",
    "val_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Create arrays to hold true and predicted values\n",
    "true_starts = []\n",
    "true_ends = []\n",
    "pred_starts = []\n",
    "pred_ends = []\n",
    "\n",
    "# Progress bar for the validation loop\n",
    "loop = tqdm(val_loader, desc=\"Evaluating\")\n",
    "for batch in loop:\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        # Move batch data to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "\n",
    "        # Model predictions\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        # Store true and predicted values\n",
    "        true_starts.append(start_true.cpu().numpy())\n",
    "        true_ends.append(end_true.cpu().numpy())\n",
    "        pred_starts.append(start_pred.cpu().numpy())\n",
    "        pred_ends.append(end_pred.cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_starts = np.concatenate(true_starts)\n",
    "true_ends = np.concatenate(true_ends)\n",
    "pred_starts = np.concatenate(pred_starts)\n",
    "pred_ends = np.concatenate(pred_ends)\n",
    "\n",
    "# Function to calculate F1 score\n",
    "def calculate_f1(true_labels, predicted_labels):\n",
    "    true_pos = np.sum(true_labels == predicted_labels)\n",
    "    false_pos = np.sum((predicted_labels != true_labels) & (predicted_labels != -1))\n",
    "    false_neg = np.sum((true_labels != predicted_labels) & (true_labels != -1))\n",
    "\n",
    "    precision = true_pos / (true_pos + false_pos + 1e-9)\n",
    "    recall = true_pos / (true_pos + false_neg + 1e-9)\n",
    "    \n",
    "    return 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "\n",
    "# Calculate F1 scores for start and end predictions\n",
    "f1_start = calculate_f1(true_starts, pred_starts)\n",
    "f1_end = calculate_f1(true_ends, pred_ends)\n",
    "\n",
    "# Average F1 score\n",
    "f1_average = (f1_start + f1_end) / 2\n",
    "\n",
    "print(f\"F1 score: {f1_average:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d5644-b5ba-477c-a8da-148ff311ba6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
